# -*- coding: utf-8 -*-
"""deep-fake-voice-recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17FkzqJN7AwEp-TFzMvOk8G7lgKZsbDMF
"""

import numpy as np
import pandas as pd
import numpy as np
import pandas as pd
import os
import librosa
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import IPython
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import RandomOverSampler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, Conv2D, MaxPool2D, Flatten
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping
!pip install resampy

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

!pip install kaggle

from google.colab import files
files.upload() # Upload your kaggle.json file here
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

audio_files_path = "/kaggle/input/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/"



!kaggle datasets download -d birdy654/deep-voice-deepfake-voice-recognition

!unzip deep-voice-deepfake-voice-recognition.zip -d /content/deep-voice-deepfake-voice-recognition

real_audio = "/content/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/REAL/linus-original.wav"
fake_audio = "/content/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/FAKE/linus-to-margot.wav"

print("Real Audio:")
IPython.display.Audio(real_audio)

print("Fake Audio:")
IPython.display.Audio(fake_audio)

real_ad, real_sr = librosa.load(real_audio)
plt.figure(figsize=(12, 4))
plt.plot(real_ad)
plt.title("Real Audio Data")
plt.show()

real_spec = np.abs(librosa.stft(real_ad))
real_spec = librosa.amplitude_to_db(real_spec, ref=np.max)

plt.figure(figsize=(14, 5))
librosa.display.specshow(real_spec, sr=real_sr, x_axis="time", y_axis="log")
plt.colorbar(format="%+2.0f dB")
plt.title("Real Audio Spectogram")
plt.show()

real_mel_spect = librosa.feature.melspectrogram(y=real_ad, sr=real_sr)
real_mel_spect = librosa.power_to_db(real_mel_spect, ref=np.max)

plt.figure(figsize=(14, 5))
librosa.display.specshow(real_mel_spect, y_axis="mel", x_axis="time")
plt.title("Real Audio Mel Spectogram")
plt.colorbar(format="%+2.0f dB")
plt.show()

real_chroma = librosa.feature.chroma_cqt(y=real_ad, sr=real_sr, bins_per_octave=36)

plt.figure(figsize=(14, 5))
librosa.display.specshow(real_chroma, sr=real_sr, x_axis="time", y_axis="chroma", vmin=0, vmax=1)
plt.colorbar()
plt.title("Real Audio Chromagram")
plt.show()

real_mfccs = librosa.feature.mfcc(y=real_ad, sr=real_sr)

plt.figure(figsize=(14, 5))
librosa.display.specshow(real_mfccs, sr=real_sr, x_axis="time")
plt.colorbar()
plt.title("Real Audio Mel-Frequency Cepstral Coefficients (MFCCs)")
plt.show()

fake_ad, fake_sr = librosa.load(fake_audio)
plt.figure(figsize=(12, 4))
plt.plot(fake_ad)
plt.title("Fake Audio Data")
plt.show()

fake_spec = np.abs(librosa.stft(fake_ad))
fake_spec = librosa.amplitude_to_db(fake_spec, ref=np.max)

plt.figure(figsize=(14, 5))
librosa.display.specshow(fake_spec, sr=fake_sr, x_axis="time", y_axis="log")
plt.colorbar(format="%+2.0f dB")
plt.title("Fake Audio Spectogram")
plt.show()

fake_mel_spect = librosa.feature.melspectrogram(y=fake_ad, sr=fake_sr)
fake_mel_spect = librosa.power_to_db(fake_mel_spect, ref=np.max)

plt.figure(figsize=(14, 5))
librosa.display.specshow(fake_mel_spect, y_axis="mel", x_axis="time")
plt.title("Fake Audio Mel Spectogram")
plt.colorbar(format="%+2.0f dB")
plt.show()

fake_chroma = librosa.feature.chroma_cqt(y=fake_ad, sr=fake_sr, bins_per_octave=36)

plt.figure(figsize=(14, 5))
librosa.display.specshow(fake_chroma, sr=fake_sr, x_axis="time", y_axis="chroma", vmin=0, vmax=1)
plt.colorbar()
plt.title("Fake Audio Chromagram")
plt.show()

fake_mfccs = librosa.feature.mfcc(y=fake_ad, sr=fake_sr)

plt.figure(figsize=(14, 5))
librosa.display.specshow(fake_mfccs, sr=fake_sr, x_axis="time")
plt.colorbar()
plt.title("Fake Audio Mel-Frequency Cepstral Coefficients (MFCCs)")
plt.show()

# Define a function to extract features (e.g., MFCCs)
def extract_features(file_path, n_mfcc=40):
    try:
        # Load the audio file
        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')
        # Extract MFCCs
        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)
        # Calculate the mean of the MFCCs
        mfccs_processed = np.mean(mfccs.T, axis=0)
    except Exception as e:
        print(f"Error processing file {file_path}: {e}")
        return None
    return mfccs_processed

# Initialize lists to store features and labels
data = []
labels = []

# Define the paths to the REAL and FAKE audio directories
real_audio_dir = "/content/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/REAL/"
fake_audio_dir = "/content/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/FAKE/"

# Process REAL audio files
print("Processing REAL audio files...")
for filename in tqdm(os.listdir(real_audio_dir)):
    if filename.endswith(".wav"):
        file_path = os.path.join(real_audio_dir, filename)
        features = extract_features(file_path)
        if features is not None:
            data.append(features)
            labels.append("REAL")

# Process FAKE audio files
print("Processing FAKE audio files...")
for filename in tqdm(os.listdir(fake_audio_dir)):
    if filename.endswith(".wav"):
        file_path = os.path.join(fake_audio_dir, filename)
        features = extract_features(file_path)
        if features is not None:
            data.append(features)
            labels.append("FAKE")

# Create the DataFrame
feature_df = pd.DataFrame({"features": data, "class": labels})

# Now you can continue with your existing code for value_counts, label encoding, etc.
feature_df["class"].value_counts()

def label_encoder(column):
    le = LabelEncoder().fit(column)
    print(column.name, le.classes_)
    return le.transform(column)

feature_df["class"] = label_encoder(feature_df["class"])

X = np.array(feature_df["features"].tolist())
y = np.array(feature_df["class"].tolist())

ros = RandomOverSampler(random_state=42)

# Check if the DataFrame is empty before resampling
if feature_df.empty:
    print("feature_df is empty. Cannot perform resampling.")
else:
    X_resampled, y_resampled = ros.fit_resample(X, y)
    print(f"Resampled data shape: X_resampled={X_resampled.shape}, y_resampled={y_resampled.shape}")

y_resampled = to_categorical(y_resampled)

X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

num_labels = len(feature_df["class"].unique())
num_labels

input_shape = feature_df["features"][0].shape
input_shape

"""# Model"""

model = Sequential()
model.add(Dense(128, input_shape=input_shape))
model.add(Activation(activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(256))
model.add(Activation(activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(128))
model.add(Activation(activation="relu"))
model.add(Dropout(0.5))
model.add(Dense(num_labels))
model.add(Activation(activation="softmax"))

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

model.summary()

history = model.fit(X_train, y_train, validation_data=(X_test, y_test), batch_size=2, epochs=100)

test_loss, test_acc = model.evaluate(X_test, y_test)
print("Test Loss:", test_loss)
print("Test Accuracy:", test_acc)

plt.figure()
plt.title("Model Accuracy")
plt.plot(history.history["accuracy"], label="train")
plt.plot(history.history["val_accuracy"], label="validation")
plt.legend()
plt.ylim([0, 1])
plt.show()

plt.figure()
plt.title("Model Loss")
plt.plot(history.history["loss"], label="train")
plt.plot(history.history["val_loss"], label="validation")
plt.legend()
plt.ylim([0, 1])
plt.show()

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, GlobalAveragePooling1D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import numpy as np
import librosa
import os
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from imblearn.over_sampling import RandomOverSampler
from tensorflow.keras.callbacks import EarlyStopping # Assuming you have early_stopping defined
import matplotlib.pyplot as plt # Import matplotlib for plotting

# Ensure necessary packages are installed
# !pip install resampy
# !pip install imblearn
# !pip install tensorflow
# !pip install scikit-learn

# --- Feature Extraction (Same as before, returns sequence) ---
def extract_features_sequence(file_path, n_mfcc=40, max_pad_len=174): # Adjust max_pad_len based on your data
    try:
        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')
        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)

        # Pad or truncate to max_pad_len
        if mfccs.shape[1] < max_pad_len:
            pad_width = max_pad_len - mfccs.shape[1]
            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')
        elif mfccs.shape[1] > max_pad_len:
            mfccs = mfccs[:, :max_pad_len]

    except Exception as e:
        print(f"Error processing file {file_path}: {e}")
        return None
    return mfccs.T # Return the transpose so the shape is (time_steps, n_mfcc)

# --- Data Loading and Preparation ---
# Initialize lists to store features and labels (sequence data)
data_sequence = []
labels_sequence = []

# Define the paths to the REAL and FAKE audio directories
real_audio_dir = "/content/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/REAL/"
fake_audio_dir = "/content/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/FAKE/"

# Process REAL audio files
print("Processing REAL audio files...")
for filename in tqdm(os.listdir(real_audio_dir)):
    if filename.endswith(".wav"):
        file_path = os.path.join(real_audio_dir, filename)
        features = extract_features_sequence(file_path)
        if features is not None:
            data_sequence.append(features)
            labels_sequence.append("REAL")

# Process FAKE audio files
print("Processing FAKE audio files...")
for filename in tqdm(os.listdir(fake_audio_dir)):
    if filename.endswith(".wav"):
        file_path = os.path.join(fake_audio_dir, filename)
        features = extract_features_sequence(file_path)
        if features is not None:
            data_sequence.append(features)
            labels_sequence.append("FAKE")

# Convert to numpy arrays
X_sequence = np.array(data_sequence)
y_sequence = np.array(labels_sequence)

# Encode labels
le = LabelEncoder().fit(y_sequence)
y_encoded_sequence = le.transform(y_sequence)

# Convert to categorical for model output
y_categorical_sequence = to_categorical(y_encoded_sequence)

# Resample the data (if needed, RandomOverSampler works with sequences too)
# Check if there are samples before resampling
if X_sequence.shape[0] > 0:
    ros = RandomOverSampler(random_state=42)
    # Reshape X_sequence for resampling as it expects 2D input
    X_sequence_reshaped = X_sequence.reshape(X_sequence.shape[0], -1)
    X_resampled_reshaped, y_resampled_encoded = ros.fit_resample(X_sequence_reshaped, y_encoded_sequence)

    # Reshape X_resampled back to sequence shape
    X_resampled_sequence = X_resampled_reshaped.reshape(X_resampled_reshaped.shape[0], X_sequence.shape[1], X_sequence.shape[2])
    y_resampled_categorical = to_categorical(y_resampled_encoded)

    # Split the data
    X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(
        X_resampled_sequence, y_resampled_categorical, test_size=0.2, random_state=49
    ) # Changed random state to try a different split
else:
    print("No audio data loaded. Cannot proceed with training.")
    # Set empty variables to avoid errors later if training is skipped
    X_train_seq, X_test_seq, y_train_seq, y_test_seq = np.array([]), np.array([]), np.array([]), np.array([])
    sequence_length = 0
    feature_dimension = 0
    num_classes = 0


# Only proceed with model definition and training if data was loaded
if X_train_seq.shape[0] > 0:
    # Define Transformer Model Parameters
    sequence_length = X_train_seq.shape[1]
    feature_dimension = X_train_seq.shape[2]
    num_classes = y_train_seq.shape[1]
    num_heads = 4  # Number of attention heads
    ff_dim = 32  # Hidden layer size in feedforward network inside transformer
    dropout_rate = 0.2 # Increased dropout rate
    num_transformer_blocks = 1 # Reduced number of transformer blocks

    # --- Transformer Block ---
    def transformer_block(inputs, num_heads, ff_dim, dropout_rate):
        # Multi-Head Self-Attention
        attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=feature_dimension)(inputs, inputs)
        attn_output = Dropout(dropout_rate)(attn_output)
        attn_output = LayerNormalization(epsilon=1e-6)(inputs + attn_output)

        # Feed Forward Network
        ff_output = Dense(ff_dim, activation="relu")(attn_output)
        ff_output = Dense(feature_dimension)(ff_output)
        ff_output = Dropout(dropout_rate)(ff_output)
        ff_output = LayerNormalization(epsilon=1e-6)(attn_output + ff_output)
        return ff_output

    # --- Positional Embedding ---
    class PositionalEmbedding(tf.keras.layers.Layer):
        def __init__(self, sequence_length, input_dim, **kwargs):
            super().__init__(**kwargs)
            self.sequence_length = sequence_length
            self.input_dim = input_dim
            self.position_embedding = tf.keras.layers.Embedding(
                input_dim=sequence_length, output_dim=input_dim
            )

        def call(self, inputs):
            # Assuming inputs shape is (batch_size, sequence_length, feature_dimension)
            positions = tf.range(start=0, limit=self.sequence_length, delta=1)
            embedded_positions = self.position_embedding(positions)
            # Ensure embedded_positions shape matches inputs shape for addition
            embedded_positions = tf.cast(embedded_positions, inputs.dtype)
            return inputs + embedded_positions

        def get_config(self):
            config = super().get_config()
            config.update({
                "sequence_length": self.sequence_length,
                "input_dim": self.input_dim,
            })
            return config


    # --- Build the Transformer Model ---
    inputs = Input(shape=(sequence_length, feature_dimension))
    x = PositionalEmbedding(sequence_length, feature_dimension)(inputs)
    x = Dropout(dropout_rate)(x) # Apply dropout after positional embedding

    for _ in range(num_transformer_blocks):
        x = transformer_block(x, num_heads, ff_dim, dropout_rate)

    # Classification Head
    x = GlobalAveragePooling1D()(x) # Global average pooling across the sequence dimension
    x = Dense(64, activation="relu")(x)
    x = Dropout(0.4)(x) # Increased dropout in the dense layer
    outputs = Dense(num_classes, activation="softmax")(x)

    transformer_model = Model(inputs=inputs, outputs=outputs)

    # Compile the model
    transformer_model.compile(
        loss="categorical_crossentropy", optimizer=Adam(learning_rate=1e-4), metrics=["accuracy"]
    )

    # Model Summary
    transformer_model.summary()

    # Define the EarlyStopping callback
    # Monitor 'val_loss' and stop training if it doesn't improve for 'patience' epochs.
    # restore_best_weights=True ensures the model uses the weights from the best epoch.
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=10,  # Number of epochs with no improvement after which training will be stopped.
        restore_best_weights=True
    )

    # Train the model
    history_transformer = transformer_model.fit(
        X_train_seq,
        y_train_seq,
        validation_data=(X_test_seq, y_test_seq),
        batch_size=32, # Batch size might need adjustment
        epochs=50,   # Reduced epochs as EarlyStopping will handle it
        callbacks=[early_stopping] # Add EarlyStopping callback
    )

    # --- Plotting ---
    # Assuming the model training was successful and history_transformer object exists
    if 'history_transformer' in locals():
        # Plot Training and Validation Accuracy
        plt.figure(figsize=(10, 6))
        plt.plot(history_transformer.history['accuracy'], label='Train Accuracy')
        plt.plot(history_transformer.history['val_accuracy'], label='Validation Accuracy')
        plt.title('Model Accuracy over Epochs (Transformer Model)')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plt.show()

        # Plot Training and Validation Loss
        plt.figure(figsize=(10, 6))
        plt.plot(history_transformer.history['loss'], label='Train Loss')
        plt.plot(history_transformer.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss over Epochs (Transformer Model)')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        plt.show()

else:
    print("Skipping model definition, training, and plotting due to no data.")

"""## **Ensemble**"""

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, GlobalAveragePooling1D, LSTM, concatenate, BatchNormalization
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l2 # Import L2 regularizer
import numpy as np
import librosa
import os
from tqdm import tqdm
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
from imblearn.over_sampling import RandomOverSampler
from tensorflow.keras.callbacks import EarlyStopping # Assuming you have early_stopping defined
import matplotlib.pyplot as plt # Import matplotlib for plotting

# Ensure necessary packages are installed
# !pip install resampy
# !pip install imblearn
# !pip install tensorflow
# !pip install scikit-learn

# --- Feature Extraction (Same as before, returns sequence) ---
def extract_features_sequence(file_path, n_mfcc=40, max_pad_len=174): # Adjust max_pad_len based on your data
    try:
        audio, sample_rate = librosa.load(file_path, res_type='kaiser_fast')
        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=n_mfcc)

        # Pad or truncate to max_pad_len
        if mfccs.shape[1] < max_pad_len:
            pad_width = max_pad_len - mfccs.shape[1]
            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')
        elif mfccs.shape[1] > max_pad_len:
            mfccs = mfccs[:, :max_pad_len]

    except Exception as e:
        print(f"Error processing file {file_path}: {e}")
        return None
    return mfccs.T # Return the transpose so the shape is (time_steps, n_mfcc)

# --- Data Loading and Preparation (Same as before) ---
# Initialize lists to store features and labels (sequence data)
data_sequence = []
labels_sequence = []

# Define the paths to the REAL and FAKE audio directories
real_audio_dir = "/content/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/REAL/"
fake_audio_dir = "/content/deep-voice-deepfake-voice-recognition/KAGGLE/AUDIO/FAKE/"

# Process REAL audio files
print("Processing REAL audio files for sequence data...")
for filename in tqdm(os.listdir(real_audio_dir)):
    if filename.endswith(".wav"):
        file_path = os.path.join(real_audio_dir, filename)
        features = extract_features_sequence(file_path)
        if features is not None:
            data_sequence.append(features)
            labels_sequence.append("REAL")

# Process FAKE audio files
print("Processing FAKE audio files for sequence data...")
for filename in tqdm(os.listdir(fake_audio_dir)):
    if filename.endswith(".wav"):
        file_path = os.path.join(fake_audio_dir, filename)
        features = extract_features_sequence(file_path)
        if features is not None:
            data_sequence.append(features)
            labels_sequence.append("FAKE")

# Convert to numpy arrays
X_sequence = np.array(data_sequence)
y_sequence = np.array(labels_sequence)

# Encode labels
le = LabelEncoder().fit(y_sequence)
y_encoded_sequence = le.transform(y_sequence)
y_categorical_sequence = to_categorical(y_encoded_sequence)

# Resample the data
ros = RandomOverSampler(random_state=42)
# Check if there are samples before resampling
if X_sequence.shape[0] > 0:
    # Reshape X_sequence for resampling as it expects 2D input
    X_sequence_reshaped = X_sequence.reshape(X_sequence.shape[0], -1)
    X_resampled_reshaped, y_resampled_encoded = ros.fit_resample(X_sequence_reshaped, y_encoded_sequence)

    # Reshape X_resampled back to sequence shape
    X_resampled_sequence = X_resampled_reshaped.reshape(X_resampled_reshaped.shape[0], X_sequence.shape[1], X_sequence.shape[2])
    y_resampled_categorical = to_categorical(y_resampled_encoded)

    # Split the data
    X_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(
        X_resampled_sequence, y_resampled_categorical, test_size=0.2, random_state=42
    ) # Changed random state to try a different split
else:
    print("No audio data loaded. Cannot proceed with training.")
    # Set empty variables to avoid errors later if training is skipped
    X_train_seq, X_test_seq, y_train_seq, y_test_seq = np.array([]), np.array([]), np.array([]), np.array([])
    sequence_length = 0
    feature_dimension = 0
    num_classes = 0


# Only proceed with model definition and training if data was loaded
if X_train_seq.shape[0] > 0:

    # --- Transformer Block (Adding BatchNormalization) ---
    def transformer_block(inputs, num_heads, ff_dim, dropout_rate):
        # Multi-Head Self-Attention
        input_dim = tf.keras.backend.int_shape(inputs)[-1]
        attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_dim)(inputs, inputs)
        attn_output = Dropout(dropout_rate)(attn_output)
        attn_output = LayerNormalization(epsilon=1e-6)(inputs + attn_output)

        # Feed Forward Network
        ff_output = Dense(ff_dim, activation="relu", kernel_regularizer=l2(1e-4))(attn_output) # Added L2 regularization
        ff_output = Dense(input_dim, kernel_regularizer=l2(1e-4))(ff_output) # Added L2 regularization
        ff_output = Dropout(dropout_rate)(ff_output)
        ff_output = LayerNormalization(epsilon=1e-6)(attn_output + ff_output)
        return ff_output

    # --- Positional Embedding (Same as before) ---
    class PositionalEmbedding(tf.keras.layers.Layer):
        def __init__(self, sequence_length, input_dim, **kwargs):
            super().__init__(**kwargs)
            self.sequence_length = sequence_length
            self.input_dim = input_dim
            self.position_embedding = tf.keras.layers.Embedding(
                input_dim=sequence_length, output_dim=input_dim
            )

        def call(self, inputs):
            # Assuming inputs shape is (batch_size, sequence_length, feature_dimension)
            positions = tf.range(start=0, limit=self.sequence_length, delta=1)
            embedded_positions = self.position_embedding(positions)
            # Ensure embedded_positions shape matches inputs shape for addition
            embedded_positions = tf.cast(embedded_positions, inputs.dtype)
            return inputs + embedded_positions

        def get_config(self):
            config = super().get_config()
            config.update({
                "sequence_length": self.sequence_length,
                "input_dim": self.input_dim,
            })
            return config

    # --- Define Ensemble Model Parameters ---
    sequence_length = X_train_seq.shape[1]
    feature_dimension = X_train_seq.shape[2]
    num_classes = y_train_seq.shape[1]

    # Transformer Branch Parameters
    num_heads = 4
    ff_dim = 32
    dropout_rate_transformer = 0.1
    num_transformer_blocks = 2

    # LSTM Branch Parameters
    lstm_units = 64 # Number of units in the LSTM layer
    dropout_rate_lstm = 0.1
    return_sequences_lstm = False # Set to False to get the last output, True to get full sequence

    # --- Build the Ensemble Model (Combined Architecture with Regularization) ---
    inputs = Input(shape=(sequence_length, feature_dimension))

    # Transformer Branch
    x_transformer = PositionalEmbedding(sequence_length, feature_dimension)(inputs)
    x_transformer = Dropout(dropout_rate_transformer)(x_transformer)
    # Add BatchNormalization after positional embedding and dropout
    x_transformer = BatchNormalization()(x_transformer)
    for _ in range(num_transformer_blocks):
        x_transformer = transformer_block(x_transformer, num_heads, ff_dim, dropout_rate_transformer)
    x_transformer = GlobalAveragePooling1D()(x_transformer)

    # LSTM Branch
    # Add BatchNormalization before LSTM
    x_lstm = BatchNormalization()(inputs)
    x_lstm = LSTM(lstm_units, return_sequences=return_sequences_lstm, dropout=dropout_rate_lstm, recurrent_dropout=dropout_rate_lstm)(x_lstm)


    # Combine Branches
    combined_features = concatenate([x_transformer, x_lstm])

    # Classification Head
    # Add BatchNormalization before the first Dense layer in the classification head
    x = BatchNormalization()(combined_features)
    x = Dense(64, activation="relu", kernel_regularizer=l2(1e-4))(x) # Added L2 regularization
    x = Dropout(0.5)(x)
    outputs = Dense(num_classes, activation="softmax", kernel_regularizer=l2(1e-4))(x) # Added L2 regularization

    ensemble_model = Model(inputs=inputs, outputs=outputs)

    # Compile the model
    ensemble_model.compile(
        loss="categorical_crossentropy", optimizer=Adam(learning_rate=1e-4), metrics=["accuracy"]
    )

    # Model Summary
    ensemble_model.summary()

    # Define the EarlyStopping callback
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True
    )

    # Train the model
    history_ensemble = ensemble_model.fit(
        X_train_seq, y_train_seq,
        validation_data=(X_test_seq, y_test_seq),
        batch_size=32,
        epochs=100,
        callbacks=[early_stopping] # Add EarlyStopping callback
    )

    # --- Plotting ---
    if 'history_ensemble' in locals():
        # Plot Training and Validation Accuracy
        plt.figure(figsize=(10, 6))
        plt.plot(history_ensemble.history['accuracy'], label='Train Accuracy')
        plt.plot(history_ensemble.history['val_accuracy'], label='Validation Accuracy')
        plt.title('Model Accuracy over Epochs (Ensemble Model with Regularization)')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plt.show()

        # Plot Training and Validation Loss
        plt.figure(figsize=(10, 6))
        plt.plot(history_ensemble.history['loss'], label='Train Loss')
        plt.plot(history_ensemble.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss over Epochs (Ensemble Model with Regularization)')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        plt.show()

else:
    print("Skipping model definition, training, and plotting due to no data.")

# Only proceed with model definition and training if data was loaded
if X_train_seq.shape[0] > 0:

    # --- Transformer Block (Adding BatchNormalization) ---
    def transformer_block(inputs, num_heads, ff_dim, dropout_rate):
        # Multi-Head Self-Attention
        input_dim = tf.keras.backend.int_shape(inputs)[-1]
        attn_output = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=input_dim)(inputs, inputs)
        attn_output = Dropout(dropout_rate)(attn_output)
        attn_output = LayerNormalization(epsilon=1e-6)(inputs + attn_output)

        # Feed Forward Network
        ff_output = Dense(ff_dim, activation="relu", kernel_regularizer=l2(1e-4))(attn_output) # Added L2 regularization
        ff_output = Dense(input_dim, kernel_regularizer=l2(1e-4))(ff_output) # Added L2 regularization
        ff_output = Dropout(dropout_rate)(ff_output)
        ff_output = LayerNormalization(epsilon=1e-6)(attn_output + ff_output)
        return ff_output

    # --- Positional Embedding (Same as before) ---
    class PositionalEmbedding(tf.keras.layers.Layer):
        def __init__(self, sequence_length, input_dim, **kwargs):
            super().__init__(**kwargs)
            self.sequence_length = sequence_length
            self.input_dim = input_dim
            self.position_embedding = tf.keras.layers.Embedding(
                input_dim=sequence_length, output_dim=input_dim
            )

        def call(self, inputs):
            # Assuming inputs shape is (batch_size, sequence_length, feature_dimension)
            positions = tf.range(start=0, limit=self.sequence_length, delta=1)
            embedded_positions = self.position_embedding(positions)
            # Ensure embedded_positions shape matches inputs shape for addition
            embedded_positions = tf.cast(embedded_positions, inputs.dtype)
            return inputs + embedded_positions

        def get_config(self):
            config = super().get_config()
            config.update({
                "sequence_length": self.sequence_length,
                "input_dim": self.input_dim,
            })
            return config

    # --- Define Ensemble Model Parameters ---
    sequence_length = X_train_seq.shape[1]
    feature_dimension = X_train_seq.shape[2]
    num_classes = y_train_seq.shape[1]

    # Transformer Branch Parameters
    num_heads = 4
    ff_dim = 32
    dropout_rate_transformer = 0.1
    num_transformer_blocks = 2

    # LSTM Branch Parameters
    lstm_units = 64 # Number of units in the LSTM layer
    dropout_rate_lstm = 0.1
    return_sequences_lstm = False # Set to False to get the last output, True to get full sequence

    # --- Build the Ensemble Model (Combined Architecture with Regularization) ---
    inputs = Input(shape=(sequence_length, feature_dimension))

    # Transformer Branch
    x_transformer = PositionalEmbedding(sequence_length, feature_dimension)(inputs)
    x_transformer = Dropout(dropout_rate_transformer)(x_transformer)
    # Add BatchNormalization after positional embedding and dropout
    x_transformer = BatchNormalization()(x_transformer)
    for _ in range(num_transformer_blocks):
        x_transformer = transformer_block(x_transformer, num_heads, ff_dim, dropout_rate_transformer)
    x_transformer = GlobalAveragePooling1D()(x_transformer)

    # LSTM Branch
    # Add BatchNormalization before LSTM
    x_lstm = BatchNormalization()(inputs)
    x_lstm = LSTM(lstm_units, return_sequences=return_sequences_lstm, dropout=dropout_rate_lstm, recurrent_dropout=dropout_rate_lstm)(x_lstm)


    # Combine Branches
    combined_features = concatenate([x_transformer, x_lstm])

    # Classification Head
    # Add BatchNormalization before the first Dense layer in the classification head
    x = BatchNormalization()(combined_features)
    x = Dense(64, activation="relu", kernel_regularizer=l2(1e-4))(x) # Added L2 regularization
    x = Dropout(0.5)(x)
    outputs = Dense(num_classes, activation="softmax", kernel_regularizer=l2(1e-4))(x) # Added L2 regularization

    ensemble_model = Model(inputs=inputs, outputs=outputs)

    # Compile the model
    ensemble_model.compile(
        loss="categorical_crossentropy", optimizer=Adam(learning_rate=1e-4), metrics=["accuracy"]
    )

    # Model Summary
    ensemble_model.summary()

    # Define the EarlyStopping callback
    early_stopping = EarlyStopping(
        monitor='val_loss',
        patience=5,
        restore_best_weights=True
    )

    # Train the model
    history_ensemble = ensemble_model.fit(
        X_train_seq, y_train_seq,
        validation_data=(X_test_seq, y_test_seq),
        batch_size=32,
        epochs=50,
        callbacks=[early_stopping] # Add EarlyStopping callback
    )

    # --- Plotting ---
    if 'history_ensemble' in locals():
        # Plot Training and Validation Accuracy
        plt.figure(figsize=(10, 6))
        plt.plot(history_ensemble.history['accuracy'], label='Train Accuracy')
        plt.plot(history_ensemble.history['val_accuracy'], label='Validation Accuracy')
        plt.title('Model Accuracy over Epochs (Ensemble Model with Regularization)')
        plt.xlabel('Epoch')
        plt.ylabel('Accuracy')
        plt.legend()
        plt.grid(True)
        plt.show()

        # Plot Training and Validation Loss
        plt.figure(figsize=(10, 6))
        plt.plot(history_ensemble.history['loss'], label='Train Loss')
        plt.plot(history_ensemble.history['val_loss'], label='Validation Loss')
        plt.title('Model Loss over Epochs (Ensemble Model with Regularization)')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.legend()
        plt.grid(True)
        plt.show()

else:
    print("Skipping model definition, training, and plotting due to no data.")

import matplotlib.pyplot as plt # Import matplotlib for plotting
import numpy as np # Import numpy for array manipulation
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc # Import necessary metrics and plotting functions
import seaborn as sns # Import seaborn for nicer plots

# --- Plotting (Training History) ---
# Check if the training history object exists
if 'history_ensemble' in locals():
    # Plot Training and Validation Accuracy
    plt.figure(figsize=(10, 6))
    plt.plot(history_ensemble.history['accuracy'], label='Train Accuracy')
    plt.plot(history_ensemble.history['val_accuracy'], label='Validation Accuracy')
    plt.title('Model Accuracy over Epochs (Ensemble Model with Regularization)')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Plot Training and Validation Loss
    plt.figure(figsize=(10, 6))
    plt.plot(history_ensemble.history['loss'], label='Train Loss')
    plt.plot(history_ensemble.history['val_loss'], label='Validation Loss')
    plt.title('Model Loss over Epochs (Ensemble Model with Regularization)')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
    plt.show()
else:
    print("Training history not found. Cannot plot training curves.")

# --- Evaluation and Further Visualizations ---
# Check if test data and trained model exist
if X_test_seq.shape[0] > 0 and 'ensemble_model' in locals():
    print("\nEvaluating the Ensemble Model on the Test Set:")
    test_loss, test_acc = ensemble_model.evaluate(X_test_seq, y_test_seq, verbose=0)
    print(f"Test Loss: {test_loss:.4f}")
    print(f"Test Accuracy: {test_acc:.4f}")

    # Get predictions
    y_pred_prob = ensemble_model.predict(X_test_seq)
    y_pred_classes = np.argmax(y_pred_prob, axis=1)
    y_true_classes = np.argmax(y_test_seq, axis=1)

    # --- Confusion Matrix ---
    print("\nConfusion Matrix:")
    cm = confusion_matrix(y_true_classes, y_pred_classes)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['FAKE', 'REAL'], yticklabels=['FAKE', 'REAL'])
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()

    # --- Classification Report ---
    print("\nClassification Report:")
    # Assuming class names are 'FAKE' and 'REAL' corresponding to 0 and 1
    print(classification_report(y_true_classes, y_pred_classes, target_names=['FAKE', 'REAL']))

    # --- ROC Curve and AUC (For Binary Classification) ---
    if num_classes == 2:
        print("\nROC Curve and AUC:")
        # For binary classification, the positive class is typically the one with index 1 ('REAL')
        fpr, tpr, thresholds = roc_curve(y_true_classes, y_pred_prob[:, 1])
        roc_auc = auc(fpr, tpr)

        plt.figure(figsize=(8, 6))
        plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
        plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
        plt.xlim([0.0, 1.0])
        plt.ylim([0.0, 1.05])
        plt.xlabel('False Positive Rate')
        plt.ylabel('True Positive Rate')
        plt.title('Receiver Operating Characteristic (ROC) Curve')
        plt.legend(loc="lower right")
        plt.grid(True)
        plt.show()
    else:
        print("\nROC Curve and AUC are typically for binary classification.")

else:
    print("\nCannot perform evaluation or generate further visualizations: Test data or trained model not available.")

# def detect_fake(filename):
#     sound_signal, sample_rate = librosa.load(filename, res_type="kaiser_fast")
#     mfcc_features = librosa.feature.mfcc(y=sound_signal, sr=sample_rate, n_mfcc=40)
#     mfccs_features_scaled = np.mean(mfcc_features.T, axis=0)
#     mfccs_features_scaled = mfccs_features_scaled.reshape(1, -1)
#     result_array = model.predict(mfccs_features_scaled)
#     print(result_array)
#     result_classes = ["FAKE", "REAL"]
#     result = np.argmax(result_array[0])
#     print("Result:", result_classes[result])