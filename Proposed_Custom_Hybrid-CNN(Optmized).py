# -*- coding: utf-8 -*-
"""Custom_CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uQtcOAk2QlpNbZLYx_DQjXPao093XmMJ
"""





# !pip install kaggle
# !mkdir -p ~/.kaggle
# !echo '{"username":"your_username","key":"your_api_key"}' > ~/.kaggle/kaggle.json
# !chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d xhlulu/140k-real-and-fake-faces -p /content/dataset --unzip

!kaggle datasets download -d ciplab/real-and-fake-face-detection -p /content/dataset --unzip

!pip uninstall -y jax jaxlib

import cv2
import numpy as np
from tensorflow.keras import layers
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.callbacks import Callback, ModelCheckpoint
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import metrics
import tensorflow as tf
from tqdm import tqdm

def build_model(pretrained):
    model = Sequential([
        pretrained,
        layers.GlobalAveragePooling2D(),
        layers.Dense(1, activation='sigmoid')
    ])

    model.compile(
        loss='binary_crossentropy',
        optimizer=Adam(),
        metrics=['accuracy']
    )

    return model

base_path = '/content/dataset/real_vs_fake/real-vs-fake/train'
image_gen = ImageDataGenerator(rescale=1./255.,
                               rotation_range=20,
                               #shear_range=0.2,
                               #zoom_range=0.2,
                               horizontal_flip=True)

# Fix: Remove the extra 'train/' from the directory path
train_flow = image_gen.flow_from_directory(
    base_path,  # Use the base_path directly
    target_size=(224, 224),
    batch_size=64,
    class_mode='binary'
)

image_gen1 = ImageDataGenerator(rescale=1./255.)

# Update the path to the validation directory
valid_flow = image_gen1.flow_from_directory(
    '/content/dataset/real_vs_fake/real-vs-fake/valid', # Updated path for validation data
    target_size=(224, 224),
    batch_size=64,
    class_mode='binary'
)

# Install required libraries
!pip install keras-vggface opencv-python tqdm

# Import necessary libraries
import matplotlib.pyplot as plt
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Dense, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator  # Use tensorflow.keras for ImageDataGenerator
from sklearn import metrics
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import LearningRateScheduler

# Dataset paths
base_path = '/content/dataset/real_vs_fake/real-vs-fake/'

# Data Generators
batch_size = 32  # Reduced batch size for CPU optimization
image_gen = ImageDataGenerator(rescale=1./255., shear_range=0.2, zoom_range=0.2, horizontal_flip=True)

train_flow = image_gen.flow_from_directory(
    base_path + 'train/',
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode='binary'
)

image_gen1 = ImageDataGenerator(rescale=1./255.)
valid_flow = image_gen1.flow_from_directory(
    base_path + 'valid/',
    target_size=(224, 224),
    batch_size=batch_size,
    class_mode='binary'
)
test_flow = image_gen1.flow_from_directory(
    base_path + 'test/',
    target_size=(224, 224),
    batch_size=1,
    shuffle=False,
    class_mode='binary'
)

# Model Definition
model = Sequential()
input_shape = (224, 224, 3)
activation = 'relu'
padding = 'same'
droprate = 0.1

model.add(BatchNormalization(input_shape=input_shape))
model.add(Conv2D(16, kernel_size=3, activation=activation, padding=padding))
model.add(MaxPooling2D(pool_size=2))
model.add(BatchNormalization())
model.add(Conv2D(32, kernel_size=3, activation=activation, padding=padding))
model.add(MaxPooling2D(pool_size=2))
model.add(BatchNormalization())
model.add(Dropout(droprate))
model.add(Conv2D(64, kernel_size=3, activation=activation, padding=padding))
model.add(MaxPooling2D(pool_size=2))
model.add(BatchNormalization())
model.add(Dropout(droprate))
model.add(Conv2D(128, kernel_size=3, activation=activation, padding=padding))
model.add(MaxPooling2D(pool_size=2))
model.add(BatchNormalization())
model.add(Dropout(droprate))
model.add(Conv2D(256, kernel_size=3, activation=activation, padding=padding))
model.add(MaxPooling2D(pool_size=2))
model.add(BatchNormalization())
model.add(Dropout(droprate))
model.add(GlobalAveragePooling2D())
model.add(Dense(1, activation='sigmoid'))

# Compile the Model
model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.003), metrics=['accuracy'])

# Define Learning Rate Scheduler
def warmup(epoch):
    if epoch < 3:  # First 3 epochs use a very high learning rate
        return 0.003
    else:
        return 0.001  # Reduce after warm-up

lr_callback = LearningRateScheduler(warmup)

# Train the Model
train_steps = len(train_flow)
valid_steps = len(valid_flow)

history = model.fit(
    train_flow,
    epochs=8,
    steps_per_epoch=train_steps,
    validation_data=valid_flow,
    validation_steps=valid_steps,
    callbacks=[lr_callback]
)

# Save the Model
model.save("custom_model_cpu_optimized.h5")

# Plot Training and Validation Loss/Accuracy
def plot_metrics(history):
    acc = history.history['accuracy']
    val_acc = history.history['val_accuracy']
    loss = history.history['loss']
    val_loss = history.history['val_loss']

    plt.figure(figsize=(12, 5))

    # Accuracy Plot
    plt.subplot(1, 2, 1)
    plt.plot(acc, label='Training Accuracy')
    plt.plot(val_acc, label='Validation Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()

    # Loss Plot
    plt.subplot(1, 2, 2)
    plt.plot(loss, label='Training Loss')
    plt.plot(val_loss, label='Validation Loss')
    plt.title('Training and Validation Loss')
    plt.legend()

    plt.show()

plot_metrics(history)

# Evaluate on Test Data
y_pred = model.predict(test_flow)
y_test = test_flow.classes

print("ROC AUC Score:", metrics.roc_auc_score(y_test, y_pred))
print("AP Score:", metrics.average_precision_score(y_test, y_pred))
print(metrics.classification_report(y_test, y_pred > 0.5))

# %% [Additional Visualizations for Research Paper]
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc

# Assuming y_pred and y_test are already computed from your trained model:
# y_pred = model.predict(test_flow) and y_test = test_flow.classes

# Convert model outputs to binary predictions using 0.5 as threshold
y_pred_binary = (y_pred > 0.5).astype(int)

# ----- Confusion Matrix -----
cm = confusion_matrix(y_test, y_pred_binary)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix")
plt.tight_layout()
plt.show()

# ----- ROC Curve -----
fpr, tpr, roc_thresholds = roc_curve(y_test, y_pred)
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f"ROC curve (AUC = {roc_auc:.2f})")
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label="Random guess")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Receiver Operating Characteristic (ROC) Curve")
plt.legend(loc="lower right")
plt.tight_layout()
plt.show()

# ----- Precision-Recall Curve -----
precision, recall, pr_thresholds = precision_recall_curve(y_test, y_pred)
pr_auc = auc(recall, precision)
plt.figure(figsize=(6, 5))
plt.plot(recall, precision, color='green', lw=2, label=f"PR curve (AUC = {pr_auc:.2f})")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision-Recall Curve")
plt.legend(loc="lower left")
plt.tight_layout()
plt.show()

import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt

def compute_saliency_map(model, img_array):
    img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)
    with tf.GradientTape() as tape:
        tape.watch(img_tensor)
        predictions = model(img_tensor, training=False)
        loss = predictions[:, 0]  # For binary classification

    gradients = tape.gradient(loss, img_tensor)
    saliency_map = tf.reduce_max(tf.abs(gradients), axis=-1)[0]  # Max over color channels

    # Normalize for visualization
    saliency_map = (saliency_map - tf.reduce_min(saliency_map)) / (tf.reduce_max(saliency_map) + 1e-8)

    return saliency_map.numpy()

# Example usage
# Get one image for testing (Assuming test_flow is defined)
img_batch, label_batch = next(test_flow)
img = img_batch[0]
img_array = np.expand_dims(img, axis=0) # Add batch dimension if necessary


saliency_map = compute_saliency_map(model, img_array)

plt.imshow(saliency_map, cmap='jet')
plt.axis('off')
plt.title("Saliency Map")
plt.show()

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

def integrated_gradients(model, img_array, baseline=None, steps=100):
    """Compute Integrated Gradients (IG) heatmap."""
    if baseline is None:
        baseline = np.mean(img_array, axis=(1, 2), keepdims=True)

    alphas = np.linspace(0, 1, steps)
    baseline_tensor = tf.convert_to_tensor(baseline, dtype=tf.float32)
    img_array_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)
    interpolated_images = baseline_tensor + (img_array_tensor - baseline_tensor) * alphas[:, np.newaxis, np.newaxis, np.newaxis]

    with tf.GradientTape() as tape:
        tape.watch(interpolated_images)
        predictions = model(interpolated_images)
        loss = predictions[:, 0]

    gradients = tape.gradient(loss, interpolated_images)
    avg_gradients = tf.reduce_mean(gradients, axis=0)

    ig_heatmap = (img_array - baseline) * avg_gradients

    ig_heatmap = np.abs(ig_heatmap[0])
    ig_heatmap = ig_heatmap / (np.max(ig_heatmap) + 1e-8)

    return ig_heatmap

# Get one image for testing
img_batch, label_batch = next(test_flow)
img = img_batch[0]
img_array = np.expand_dims(img, axis=0)

# Calculate the Integrated Gradients heatmap
heatmap = integrated_gradients(model, img_array, steps=100)

# Display the heatmap
plt.figure(figsize=(5, 5))
plt.imshow(heatmap, cmap="inferno")  # You can change the colormap if you prefer
plt.title("Integrated Gradients")
plt.colorbar()  # Add a colorbar to show the heatmap scale
plt.axis("off")  # Hide axis ticks and labels
plt.show()

def lrp_heatmap(model, img_array):
    """Compute Layer-wise Relevance Propagation (LRP) heatmap."""
    img_tensor = tf.convert_to_tensor(img_array, dtype=tf.float32)  # Convert to TensorFlow tensor
    with tf.GradientTape() as tape:
        tape.watch(img_tensor)  # Watch the tensor
        prediction = model(img_tensor, training=False)  # Ensure model is in inference mode
        loss = prediction[:, 0]  # For binary classification, get the positive class score

    gradients = tape.gradient(loss, img_tensor)
    if gradients is None:
        # This means the loss was not dependent on any watched tensors
        # Returning a zero array of the same shape as img_array for visualization
        print("Warning: Gradients are None. This could mean there's no dependency between the loss and the watched tensors.")
        return np.zeros_like(img_array[0])

    relevance = gradients * img_tensor  # Element-wise relevance
    relevance = np.abs(relevance[0])  # Take absolute values of the first element
    relevance /= np.max(relevance) + 1e-8  # Normalize

    return relevance

# Get one image for testing
img_batch, label_batch = next(test_flow)
img = img_batch[0]
img_array = np.expand_dims(img, axis=0)  # Add batch dimension if necessary

# Compute LRP heatmap
lrp_map = lrp_heatmap(model, img_array)

# Visualization
plt.figure(figsize=(6, 6))
plt.imshow(lrp_map, cmap="magma")
plt.title("LRP Heatmap")
plt.axis("off")
plt.colorbar()
plt.show()

import cv2

# Apply a colormap for better visualization
def enhance_heatmap(heatmap, img_array, alpha=0.5):
    """Enhance heatmap by applying color mapping and overlaying on the original image."""

    heatmap = np.uint8(255 * heatmap)  # Normalize to 0-255
    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)  # Apply colormap

    # Resize heatmap to match image dimensions
    heatmap = cv2.resize(heatmap, (img_array.shape[2], img_array.shape[1]))

    # Convert original image to 0-255 for blending
    original_image = np.uint8(255 * img_array[0])

    # Overlay heatmap on original image
    superimposed_img = cv2.addWeighted(original_image, 1 - alpha, heatmap, alpha, 0)

    return superimposed_img

# Generate the enhanced heatmap
enhanced_img = enhance_heatmap(heatmap, img_array)

# Show the final image
plt.imshow(cv2.cvtColor(enhanced_img, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB
plt.title("Enhanced Integrated Gradients Heatmap")
plt.axis("off")
plt.show()

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import cv2

def integrated_gradients(model, img_array, baseline=None, steps=200):
    """Compute Integrated Gradients (IG) with refined visualization."""

    if baseline is None:
        baseline = np.zeros_like(img_array)  # Black baseline image

    alphas = np.linspace(0, 1, steps)[:, np.newaxis, np.newaxis, np.newaxis]
    interpolated_images = baseline + alphas * (img_array - baseline)

    interpolated_images = tf.convert_to_tensor(interpolated_images, dtype=tf.float32)

    with tf.GradientTape() as tape:
        tape.watch(interpolated_images)
        predictions = model(interpolated_images)
        loss = predictions[:, tf.argmax(predictions[0])]  # Focus on highest confidence class

    gradients = tape.gradient(loss, interpolated_images)
    avg_gradients = tf.reduce_mean(gradients, axis=0).numpy()

    ig_heatmap = np.abs(avg_gradients)
    ig_heatmap /= (np.max(ig_heatmap) + 1e-8)  # Normalize

    return ig_heatmap


def overlay_heatmap(img, heatmap, colormap=cv2.COLORMAP_JET, alpha=0.5):
    """Overlay heatmap on the original image with transparency control."""
    heatmap = (heatmap * 255).astype(np.uint8)  # Scale to 0-255
    heatmap = cv2.applyColorMap(heatmap, colormap)  # Apply colormap
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)  # Convert to RGB

    overlay = cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0)  # Blend
    return overlay


# --- Load Sample Image ---
img_batch, label_batch = next(test_flow)  # Get image from test data
img = img_batch[0]
img_array = np.expand_dims(img, axis=0)

# Compute IG heatmap
heatmap = integrated_gradients(model, img_array)

# Generate refined heatmap overlay
refined_heatmap = overlay_heatmap((img * 255).astype(np.uint8), heatmap, colormap=cv2.COLORMAP_JET, alpha=0.6)

# --- Visualization ---
plt.figure(figsize=(10, 5))

plt.subplot(1, 2, 1)
plt.imshow(img_array[0])
plt.title("Original Image")
plt.axis('off')

plt.subplot(1, 2, 2)
plt.imshow(refined_heatmap)
plt.title("Refined Integrated Gradients Heatmap")
plt.axis('off')

plt.tight_layout()
plt.show()

import shap
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from shap.maskers import Image

# --- Define SHAP Prediction Function ---
def f(images):
    images = tf.convert_to_tensor(images, dtype=tf.float32)
    return model(images).numpy()

# --- Select an Image from Test Set ---
img_batch, label_batch = next(test_flow)
img = img_batch[0]
img_array = np.expand_dims(img, axis=0)  # Expand dimensions for model input

# --- Use SHAP ImageMasker ---
masker = Image("inpaint_telea", img.shape)  # Correct masker usage
explainer = shap.Explainer(f, masker)

# --- Compute SHAP Values ---
shap_values = explainer(img_array)

# --- Visualization ---
plt.figure(figsize=(8, 5))
shap.image_plot(shap_values, img_array)

!pip install lime
from lime import lime_image
from skimage.segmentation import mark_boundaries

# Create LIME explainer
explainer = lime_image.LimeImageExplainer()

# Explain the image
explanation = explainer.explain_instance(
    img.astype('double'),
    model.predict,
    top_labels=1,
    hide_color=0,
    num_samples=1000
)

# Get the explanation mask
temp, mask = explanation.get_image_and_mask(
    explanation.top_labels[0],
    positive_only=True,
    num_features=10,
    hide_rest=False
)

# --- Visualization ---
plt.figure(figsize=(8, 5))
plt.imshow(mark_boundaries(temp, mask))
plt.title("LIME Explanation")
plt.axis('off')
plt.show()

from tensorflow.keras.utils import plot_model
import matplotlib.pyplot as plt

# Save the model architecture with an increased DPI for better resolution
plot_model(model, to_file="model_architecture.png", show_shapes=True, show_layer_names=True, dpi=600)

# Load and display the image with a larger figure size
img = plt.imread("model_architecture.png")
plt.figure(figsize=(20, 20))  # Increase figure size further
plt.imshow(img)
plt.axis("off")  # Hide axes for a cleaner view
plt.show()





# %% [Grad-CAM Visualization - Updated]
import tensorflow as tf
import numpy as np
import cv2
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import array_to_img

# Function to retrieve the name of the last Conv2D layer in the model
def get_last_conv_layer(model):
    for layer in reversed(model.layers):
        if isinstance(layer, tf.keras.layers.Conv2D):
            return layer.name
    return None

# Get the last conv layer name (we ignore retrieving the last dense layer since model.output is sufficient)
last_conv_layer_name = get_last_conv_layer(model)
print("Last convolutional layer:", last_conv_layer_name)

# Print model summary for reference
model.summary()

# Function to generate Grad-CAM heatmap using the last conv layer and model's output
def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):
    # Force the model to build by doing a dummy prediction
    dummy_input = np.zeros((1, 224, 224, 3))
    model.predict(dummy_input)

    # Build a model that maps the input image to the activations of the last conv layer and the final predictions
    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(last_conv_layer_name).output, model.output]
    )

    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        loss = predictions[:, pred_index]

    # Compute gradients with respect to the last conv layer
    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    # Multiply each channel in the feature map by the corresponding pooled gradient
    conv_outputs = conv_outputs[0]
    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
    heatmap = tf.squeeze(heatmap)

    # Normalize the heatmap
    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-8)
    return heatmap.numpy()

# Get one test image from the test generator using next()
img_batch, label_batch = next(test_flow)  # Use next() for DirectoryIterator
img = img_batch[0]  # Original image (rescaled)

# Prepare image for Grad-CAM (expand dims to create a batch of 1)
img_array = np.expand_dims(img, axis=0)

# Generate the Grad-CAM heatmap
heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)

# Resize heatmap to match original image size
heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))

# Convert heatmap to 8-bit and apply a colormap
heatmap = np.uint8(255 * heatmap)
heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

# Superimpose the heatmap on the original image
# Note: Original image is in [0,1]; convert it to [0,255] for visualization.
superimposed_img = cv2.addWeighted((img * 255).astype(np.uint8), 0.6, heatmap_color, 0.4, 0)

# Plot and display the original image and Grad-CAM overlay
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
plt.imshow(img)
plt.title("Original Image")
plt.axis("off")

plt.subplot(1, 2, 2)
plt.imshow(superimposed_img)
plt.title("Grad-CAM Overlay")
plt.axis("off")
plt.show()

import tensorflow as tf
import numpy as np
import cv2
import matplotlib.pyplot as plt

# -- Helper function: Find last Conv2D layer --
def get_last_conv_layer(model):
    # Scan layers in reverse order and return the name of the first Conv2D encountered.
    for layer in reversed(model.layers):
        if isinstance(layer, tf.keras.layers.Conv2D):
            return layer.name
    raise ValueError("No Conv2D layer found in model.")

# Get last conv layer name from your model
last_conv_layer_name = get_last_conv_layer(model)
print("Last convolutional layer:", last_conv_layer_name)

# -- Force model build: Use a dummy input to ensure all layers are built --
dummy_input = np.zeros((1, 224, 224, 3))
model.predict(dummy_input)

# -- Grad-CAM Function --
def compute_gradcam(img_array, model, last_conv_layer_name, pred_index=None):
    # Build a model that outputs both the activations of the last conv layer and the model predictions.
    grad_model = tf.keras.models.Model(
        [model.inputs],
        [model.get_layer(last_conv_layer_name).output, model.output]
    )
    with tf.GradientTape() as tape:
        conv_outputs, predictions = grad_model(img_array)
        if pred_index is None:
            pred_index = tf.argmax(predictions[0])
        loss = predictions[:, pred_index]
    # Compute gradients of the loss w.r.t. conv layer outputs
    grads = tape.gradient(loss, conv_outputs)
    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))

    conv_outputs = conv_outputs[0]
    # Weight each channel in the conv output with the corresponding pooled gradient
    cam = conv_outputs @ pooled_grads[..., tf.newaxis]
    cam = tf.squeeze(cam)

    # Apply ReLU and normalize between 0 and 1
    cam = tf.maximum(cam, 0)
    cam = cam / (tf.math.reduce_max(cam) + 1e-8)
    return cam.numpy()

# -- Prepare a single test image --
# Use next() on your test_flow DirectoryIterator (batch size should be 1 for simplicity)
img_batch, _ = next(test_flow)  # Assuming test_flow defined with batch_size=1
img = img_batch[0]  # Shape: (height, width, 3) in [0,1] range

# Expand dims to create a batch of 1 for the model
img_array = np.expand_dims(img, axis=0)

# Compute Grad-CAM heatmap
heatmap = compute_gradcam(img_array, model, last_conv_layer_name)

# Resize heatmap to match original image dimensions
heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))
heatmap = np.uint8(255 * heatmap)  # Scale to [0,255]
heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

# Superimpose the heatmap on the original image
original_img_uint8 = (img * 255).astype(np.uint8)
superimposed_img = cv2.addWeighted(original_img_uint8, 0.6, heatmap_color, 0.4, 0)

# -- Save the Grad-CAM image --
output_path = "grad_cam_result.png"
cv2.imwrite(output_path, cv2.cvtColor(superimposed_img, cv2.COLOR_RGB2BGR))
print("Grad-CAM visualization saved to:", output_path)

# -- Display the result using matplotlib (static image) --
plt.figure(figsize=(12,6))
plt.subplot(1,2,1)
plt.imshow(img)
plt.title("Original Image")
plt.axis("off")

plt.subplot(1,2,2)
plt.imshow(superimposed_img)
plt.title("Grad-CAM Overlay")
plt.axis("off")
plt.tight_layout()
plt.show()

# !kaggle datasets download -d xhlulu/140k-real-and-fake-faces -p /content/dataset --unzip


# !kaggle datasets download -d ciplab/real-and-fake-face-detection -p /content/dataset --unzip


# import cv2
# import numpy as np
# from tensorflow.keras import layers
# from tensorflow.keras.applications import DenseNet121
# from tensorflow.keras.callbacks import Callback, ModelCheckpoint
# from tensorflow.keras.preprocessing.image import ImageDataGenerator
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.optimizers import Adam
# import matplotlib.pyplot as plt
# import pandas as pd
# from sklearn.model_selection import train_test_split
# from sklearn import metrics
# import tensorflow as tf
# from tqdm import tqdm


# def build_model(pretrained):
#     model = Sequential([
#         pretrained,
#         layers.GlobalAveragePooling2D(),
#         layers.Dense(1, activation='sigmoid')
#     ])

#     model.compile(
#         loss='binary_crossentropy',
#         optimizer=Adam(),
#         metrics=['accuracy']
#     )

#     return model



# base_path = '/content/dataset/real_vs_fake/real-vs-fake/train'
# image_gen = ImageDataGenerator(rescale=1./255.,
#                                rotation_range=20,
#                                #shear_range=0.2,
#                                #zoom_range=0.2,
#                                horizontal_flip=True)

# # Fix: Remove the extra 'train/' from the directory path
# train_flow = image_gen.flow_from_directory(
#     base_path,  # Use the base_path directly
#     target_size=(224, 224),
#     batch_size=64,
#     class_mode='binary'
# )


# image_gen1 = ImageDataGenerator(rescale=1./255.)

# # Update the path to the validation directory
# valid_flow = image_gen1.flow_from_directory(
#     '/content/dataset/real_vs_fake/real-vs-fake/valid', # Updated path for validation data
#     target_size=(224, 224),
#     batch_size=64,
#     class_mode='binary'
# )








# # Install required libraries
# !pip install keras-vggface opencv-python tqdm --quiet

# # Import necessary libraries
# import matplotlib.pyplot as plt
# import cv2
# import tensorflow as tf
# import numpy as np
# import seaborn as sns
# from keras.models import Sequential
# from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Dropout, Dense, BatchNormalization
# from tensorflow.keras.preprocessing.image import ImageDataGenerator
# from sklearn import metrics
# from keras.optimizers import Adam

# # Dataset paths
# base_path = '/content/dataset/real_vs_fake/real-vs-fake/'

# # Data Generators
# batch_size = 32  # Reduced batch size for CPU optimization
# image_gen = ImageDataGenerator(rescale=1./255., shear_range=0.2, zoom_range=0.2, horizontal_flip=True)
# train_flow = image_gen.flow_from_directory(
#     base_path + 'train/',
#     target_size=(224, 224),
#     batch_size=batch_size,
#     class_mode='binary'
# )

# image_gen1 = ImageDataGenerator(rescale=1./255.)
# valid_flow = image_gen1.flow_from_directory(
#     base_path + 'valid/',
#     target_size=(224, 224),
#     batch_size=batch_size,
#     class_mode='binary'
# )
# test_flow = image_gen1.flow_from_directory(
#     base_path + 'test/',
#     target_size=(224, 224),
#     batch_size=1,
#     shuffle=False,
#     class_mode='binary'
# )

# # Model Definition
# model = Sequential()
# input_shape = (224, 224, 3)
# activation = 'relu'
# padding = 'same'
# droprate = 0.1

# model.add(BatchNormalization(input_shape=input_shape))
# model.add(Conv2D(16, kernel_size=3, activation=activation, padding=padding))
# model.add(MaxPooling2D(pool_size=2))
# model.add(BatchNormalization())
# model.add(Conv2D(32, kernel_size=3, activation=activation, padding=padding))
# model.add(MaxPooling2D(pool_size=2))
# model.add(BatchNormalization())
# model.add(Dropout(droprate))
# model.add(Conv2D(64, kernel_size=3, activation=activation, padding=padding))
# model.add(MaxPooling2D(pool_size=2))
# model.add(BatchNormalization())
# model.add(Dropout(droprate))
# model.add(Conv2D(128, kernel_size=3, activation=activation, padding=padding))
# model.add(MaxPooling2D(pool_size=2))
# model.add(BatchNormalization())
# model.add(Dropout(droprate))
# model.add(Conv2D(256, kernel_size=3, activation=activation, padding=padding))
# model.add(MaxPooling2D(pool_size=2))
# model.add(BatchNormalization())
# model.add(Dropout(droprate))
# model.add(GlobalAveragePooling2D())
# model.add(Dense(1, activation='sigmoid'))

# # Compile the Model
# model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])

# # Train the Model
# train_steps = len(train_flow)
# valid_steps = len(valid_flow)
# history = model.fit(
#     train_flow,
#     epochs=4,
#     steps_per_epoch=train_steps,
#     validation_data=valid_flow,
#     validation_steps=valid_steps
# )

# # Save the Model
# model.save("custom_model_cpu_optimized.h5")

# # Plot Training and Validation Loss/Accuracy
# def plot_metrics(history):
#     acc = history.history['accuracy']
#     val_acc = history.history['val_accuracy']
#     loss = history.history['loss']
#     val_loss = history.history['val_loss']

#     plt.figure(figsize=(12, 5))
#     plt.subplot(1, 2, 1)
#     plt.plot(acc, label='Training Accuracy')
#     plt.plot(val_acc, label='Validation Accuracy')
#     plt.title('Training and Validation Accuracy')
#     plt.legend()

#     plt.subplot(1, 2, 2)
#     plt.plot(loss, label='Training Loss')
#     plt.plot(val_loss, label='Validation Loss')
#     plt.title('Training and Validation Loss')
#     plt.legend()
#     plt.show()

# plot_metrics(history)

# # Evaluate on Test Data
# y_pred = model.predict(test_flow)
# y_test = test_flow.classes

# print("ROC AUC Score:", metrics.roc_auc_score(y_test, y_pred))
# print("AP Score:", metrics.average_precision_score(y_test, y_pred))
# print(metrics.classification_report(y_test, y_pred > 0.5))

# # -------------------- XAI: Grad-CAM --------------------

# def get_gradcam_heatmap(model, image, last_conv_layer_index, pred_index=None):
#     grad_model = tf.keras.models.Model(
#         [model.inputs],
#         [model.layers[last_conv_layer_index].output, model.output]
#     )
#     with tf.GradientTape() as tape:
#         conv_outputs, predictions = grad_model(np.expand_dims(image, axis=0))
#         if pred_index is None:
#             pred_index = tf.argmax(predictions[0])
#         class_channel = predictions[:, pred_index]
#     grads = tape.gradient(class_channel, conv_outputs)
#     pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))
#     conv_outputs = conv_outputs[0]
#     heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]
#     heatmap = tf.squeeze(heatmap)
#     heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-8)
#     return heatmap.numpy()

# def overlay_heatmap_on_image(image, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):
#     heatmap = cv2.resize(heatmap, (image.shape[1], image.shape[0]))
#     heatmap = np.uint8(255 * heatmap)
#     heatmap = cv2.applyColorMap(heatmap, colormap)
#     output_image = cv2.addWeighted(image, 1 - alpha, heatmap, alpha, 0)
#     return output_image

# # Select one test image from test_flow (batch_size=1)
# test_batch = next(test_flow)
# test_image = test_batch[0][0]  # Extract the first image from the batch

# # For your model, the last Conv2D layer is at index 16.
# last_conv_layer_index = 16
# heatmap = get_gradcam_heatmap(model, test_image, last_conv_layer_index)

# orig_image = np.uint8(test_image * 255)  # Convert from normalized [0,1] to [0,255]
# overlay = overlay_heatmap_on_image(orig_image, heatmap, alpha=0.4)

# plt.figure(figsize=(15, 5))
# plt.subplot(1, 3, 1)
# plt.imshow(orig_image)
# plt.title("Original Image")
# plt.axis("off")
# plt.subplot(1, 3, 2)
# plt.imshow(heatmap, cmap='jet')
# plt.title("Grad-CAM Heatmap")
# plt.axis("off")
# plt.subplot(1, 3, 3)
# plt.imshow(cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB))
# plt.title("Overlay")
# plt.axis("off")
# plt.suptitle("Explainable AI (XAI) using Grad-CAM", fontsize=16)
# plt.show()



# !pip install kaggle
# !mkdir -p ~/.kaggle
# !echo '{"username":"your_username","key":"your_api_key"}' > ~/.kaggle/kaggle.json
# !chmod 600 ~/.kaggle/kaggle.json

# Install required libraries
# !pip install tensorflow opencv-python tqdm scikit-learn matplotlib seaborn

# import tensorflow as tf
# from tensorflow.keras.applications import MobileNetV2
# from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
# from tensorflow.keras.models import Model
# from tensorflow.keras.preprocessing.image import ImageDataGenerator
# from tensorflow.keras.optimizers import Adam
# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
# import matplotlib.pyplot as plt
# from sklearn import metrics
# from sklearn.metrics import confusion_matrix
# import seaborn as sns

# # -------------------------------
# # 1. Data Preparation
# # -------------------------------
# base_path = '/content/dataset/real_vs_fake/real-vs-fake/'
# batch_size = 32
# target_size = (224, 224)

# # Aggressive data augmentation for training
# train_datagen = ImageDataGenerator(
#     rescale=1./255.,
#     shear_range=0.2,
#     zoom_range=0.2,
#     horizontal_flip=True,
#     rotation_range=20,
#     width_shift_range=0.2,
#     height_shift_range=0.2
# )
# # Simple rescaling for validation and test
# val_datagen = ImageDataGenerator(rescale=1./255.)
# test_datagen = ImageDataGenerator(rescale=1./255.)

# train_flow = train_datagen.flow_from_directory(
#     base_path + 'train/',
#     target_size=target_size,
#     batch_size=batch_size,
#     class_mode='binary'
# )
# val_flow = val_datagen.flow_from_directory(
#     base_path + 'valid/',
#     target_size=target_size,
#     batch_size=batch_size,
#     class_mode='binary'
# )
# test_flow = test_datagen.flow_from_directory(
#     base_path + 'test/',
#     target_size=target_size,
#     batch_size=1,
#     shuffle=False,
#     class_mode='binary'
# )

# # -------------------------------
# # 2. Build the Model using MobileNetV2 (Transfer Learning)
# # -------------------------------
# # Load the base model with pretrained ImageNet weights.
# base_model = MobileNetV2(include_top=False, weights='imagenet', input_shape=(224,224,3))
# base_model.trainable = False  # Freeze for initial training

# # Add a custom classification head.
# x = base_model.output
# x = GlobalAveragePooling2D()(x)
# x = Dropout(0.5)(x)  # Use high dropout to help generalization
# predictions = Dense(1, activation='sigmoid')(x)
# model = Model(inputs=base_model.input, outputs=predictions)

# # Compile the model (initial phase)
# model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])

# # -------------------------------
# # 3. Callbacks & Training Settings
# # -------------------------------
# callbacks = [
#     EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True, verbose=1),
#     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1),
#     ModelCheckpoint("best_model.h5", monitor='val_loss', save_best_only=True, verbose=1)
# ]

# # Limit steps per epoch to use only a subset of the full generator (reduce from ~3125 to 2000)
# steps_per_epoch = 2000
# validation_steps = min(50, len(val_flow))  # Adjust as needed

# # -------------------------------
# # 4. Phase 1: Feature Extraction (Frozen Base)
# # -------------------------------
# initial_epochs = 5
# history_initial = model.fit(
#     train_flow,
#     epochs=initial_epochs,
#     steps_per_epoch=steps_per_epoch,
#     validation_data=val_flow,
#     validation_steps=validation_steps,
#     callbacks=callbacks,
#     verbose=1
# )

# # -------------------------------
# # 5. Phase 2: Fine-Tuning (Unfreeze Last Layers)
# # -------------------------------
# # Unfreeze the last 30 layers of the base model for fine-tuning.
# base_model.trainable = True
# for layer in base_model.layers[:-30]:
#     layer.trainable = False

# # Recompile with a lower learning rate for fine-tuning.
# model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

# # Increase fine-tuning epochs so that total epochs = initial_epochs + fine_tune_epochs = 5 + 15 = 20
# fine_tune_epochs = 15
# total_epochs = initial_epochs + fine_tune_epochs
# history_fine = model.fit(
#     train_flow,
#     epochs=total_epochs,
#     initial_epoch=initial_epochs,
#     steps_per_epoch=steps_per_epoch,
#     validation_data=val_flow,
#     validation_steps=validation_steps,
#     callbacks=callbacks,
#     verbose=1
# )

# model.save("mobilenetv2_finetuned.h5")

# # -------------------------------
# # 6. Plot Training Metrics
# # -------------------------------
# # Combine histories for plotting
# train_acc = history_initial.history['accuracy'] + history_fine.history['accuracy']
# val_acc = history_initial.history['val_accuracy'] + history_fine.history['val_accuracy']
# train_loss = history_initial.history['loss'] + history_fine.history['loss']
# val_loss = history_initial.history['val_loss'] + history_fine.history['val_loss']

# plt.figure(figsize=(12, 5))
# plt.subplot(1, 2, 1)
# plt.plot(train_acc, label='Train Accuracy')
# plt.plot(val_acc, label='Validation Accuracy')
# plt.title('Training and Validation Accuracy')
# plt.legend()

# plt.subplot(1, 2, 2)
# plt.plot(train_loss, label='Train Loss')
# plt.plot(val_loss, label='Validation Loss')
# plt.title('Training and Validation Loss')
# plt.legend()
# plt.show()

# # -------------------------------
# # 7. Evaluate on Test Data & Generate Confusion Matrix
# # -------------------------------
# y_pred = model.predict(test_flow)
# y_test = test_flow.classes
# roc_auc = metrics.roc_auc_score(y_test, y_pred)
# ap_score = metrics.average_precision_score(y_test, y_pred)

# print("ROC AUC Score:", roc_auc)
# print("Average Precision Score:", ap_score)
# print(metrics.classification_report(y_test, y_pred > 0.5))

# # Confusion Matrix
# y_pred_class = (y_pred > 0.5).astype("int32")
# cm = confusion_matrix(y_test, y_pred_class)
# print("Confusion Matrix:\n", cm)

# plt.figure(figsize=(6,5))
# sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
# plt.xlabel("Predicted Label")
# plt.ylabel("True Label")
# plt.title("Confusion Matrix")
# plt.show()



